{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Build a RAG Application with LangChain, Part 1\n",
    "\n",
    "Like with [Lab 1](./1_search.ipynb) you will use the transcripts from the [Boston Azure Youtube channel](https://www.youtube.com/bostonazure)(included in this repo) to create a RAG application with LangChain. \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Load the environment variables needed from the .env file (this assumes you are running this in VS Code)\n",
    "* Learn how to interact with AzureOpenAI using LangChain (not the AzureOpenAI client like we did in Lab 1)\n",
    "* Learn the features of LangChain needed to implement a RAG application\n",
    "\n",
    "## Goals\n",
    "1. Create functionality to provide a text query\n",
    "2. Have an in-memory vector store searched for the most similar video transcripts\n",
    "3. Send a prompt to OpenAI to then get a response to the query\n",
    "\n",
    "> NOTE:\n",
    "> \n",
    "> There is no web UI to this \"application\" we are going to build - only this notebook.\n",
    ">\n",
    "\n",
    "### Step 1: Load environment variables and setup LangChain to use AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "  openai_api_version=\"2023-05-15\",\n",
    "  azure_deployment= os.getenv(\"AZURE_OPENAI_MODEL_DEPLOYMENT_NAME\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangChain site has more information on [how to connect to an Azure OpenAI deployment](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/) - though some code on that page is a little out of date, so I typically look at the [OpenAI docs](https://python.langchain.com/docs/integrations/chat/openai/) for general usage since it tends to be more up to date.\n",
    "\n",
    "### Step 2: Use LangChain to interact with the AzureOpenAI chat model\n",
    "\n",
    "In order to verify the model is setup correctly, we can test by making a call with the invoke method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What is MIT?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the [LangChain ChatOpenAI docs](https://python.langchain.com/docs/integrations/chat/openai/), you'll see we can also follow a more typical chat model having both a system message and a human message, like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant that is very brief but polite in your answers. Answer questions in less than 50 words.\"),\n",
    "    (\"human\", \"What is MIT?\")\n",
    "]\n",
    "\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed the response from the model is an [AIMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html#langchain_core.messages.ai.AIMessage). However, we can convert the output to a just a string response using the [StrOutputParser](https://python.langchain.com/docs/modules/model_io/concepts/#stroutputparser). \n",
    "\n",
    "#### Building a chain to format the output from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# combine the llm and parser into a chain\n",
    "chain = llm | parser\n",
    "chain.invoke(\"What is MIT?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to more specific with the llm call, we can also pass a system and human message, instead of just the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant that is very brief but polite in your answers. Answer questions in less than 50 words.\"),\n",
    "    (\"human\", \"What is MIT?\")\n",
    "]\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt templates\n",
    "\n",
    "So far we've called the LLM in two ways:\n",
    "1. Passing a single text message\n",
    "2. Passing a system and human message\n",
    "\n",
    "Now let's take a step closer to what we want to do for the RAG pattern later on, which is passing **context** along with our message to the LLM. To do this we'll use the [ChatPromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/#chatprompttemplate).\n",
    "\n",
    "To start with, let's use just a single message that has placeholder for parameters to be plugged in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Harvard is in Cambridge\", question=\"Where is Harvard?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, since we didn't specify how the message should be categorized, it is defaulting to a human message for the string we provided. That should be fine for now.\n",
    "\n",
    "Next, we add the prompt as the first item in our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"Harvard is in Cambridge\",\n",
    "    \"question\": \"Where is Harvard?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the chain above do? Sometimes it is easiest to read backwards:\n",
    "* the parser takes the output from the llm\n",
    "* the llm takes the output from the prompt\n",
    "* the invoke passes in the parameters - in this case the parameters needed by the ChatPromptTemplate\n",
    "\n",
    "The result is the llm's answer to the question taking into account the prompt which instructed the llm to use the context. This is RAG in it's most simple form.\n",
    "\n",
    "Now let's do the same thing passing a system and human message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are a helpful assistant that is very brief but polite in your answers. Answer questions in less than 50 words.\n",
    "            Answer the question based on the context below. If you can't \n",
    "            answer the question, reply \"I don't know\".\n",
    "\n",
    "            Context: {context}\n",
    "         \"\"\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ],\n",
    ")\n",
    "prompt_template.format_messages(context=\"Harvard is in Cambridge\", question=\"Where is Harvard?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"Harvard is in Cambridge\",\n",
    "    \"question\": \"Where is Harvard?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got an idea of how to make calls to the LLM with LangChain, its time to look at the transcript file for this lab.\n",
    "\n",
    "### Step 3: Load the transcript file (and take a look at what is in it)\n",
    "\n",
    "First set the file we want to use and then use pandas to load load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"./prep/output/master_transcriptions.json\"\n",
    "\n",
    "import pandas as pd\n",
    "transcripts_dataset = pd.read_json(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at what is in the data frame. Unlike the transcript file we used in Lab 1, this file does not have the embeddings already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain has many different [document loaders](https://python.langchain.com/docs/integrations/document_loaders/), since we are already familiar with loading our transcript file using pandas it makes sense to use the [Pandas DataFrame](https://python.langchain.com/docs/integrations/document_loaders/pandas_dataframe/) loader.\n",
    "\n",
    "One of the things you can specify when using the DataFrameLoader is the **page_content_column**. In this case we'll use the text column (which is the transcript text). All additional columns will be added as metadata.\n",
    "\n",
    "You may be wondering why we need a document loader ...\n",
    "\n",
    "In this case, it is because we are planning to use the [DocArrayInMemorySearch](https://python.langchain.com/docs/integrations/vectorstores/docarray_in_memory/#using-docarrayinmemorysearch) for an in memory vector store later in this lab. \n",
    "\n",
    "Load the data frame with DataFrameLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "# load the dataset and specify to use the transcript text column for the page content\n",
    "loader = DataFrameLoader(transcripts_dataset, page_content_column=\"text\")\n",
    "transcripts = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings\n",
    "\n",
    "In [Lab 1](./1_search.ipynb) we didn't need to get the embeddings because they were already in the transcript file, however for this lab we don't have the embeddings **and** we want to see how the vector store can do the work of calling AzureOpenAI to do that for us.\n",
    "\n",
    "LangChain documentation on [AzureOpenAIEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/azureopenai/) doesn't quite mention it but the [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.openai.OpenAIEmbeddings.html#langchain_community.embeddings.openai.OpenAIEmbeddings) API reference does, if you don't pass a deployment parameter it will use the default 'text-embedding-ada-002' which is what I used.\n",
    "\n",
    "Set the embeddings variable to use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the embeddings model is being called by passing it a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = embeddings.embed_query(\"Where is MIT?\")\n",
    "\n",
    "# for text-embedding-ada-002 the correct number is 1536\n",
    "print(f\"Embedding length: {len(embedded_query)}\")\n",
    "\n",
    "# verify contents looks normal but shorten to only show 10\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at embeddings and their similarity (this time we use another package not calculate one like we did in Lab 1).\n",
    "\n",
    "Get the embedding for two sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = embeddings.embed_query(\"MIT is in Cambridge\")\n",
    "sentence2 = embeddings.embed_query(\"Cambridge is across the river from Boston\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `sklearn.metrics.pairwise` package's cosine_similarity calculate their values and output them to see how close in meaning the two sentences are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
    "\n",
    "query_sentence1_similarity, query_sentence2_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-Memory Vector Store\n",
    "\n",
    "As I mentioned at the beginning of this lab, in order to get our RAG pattern implemented we are using an in-memory store here. LangChain offers different options but for this lab we are going to use the [DocArrayInMemorySearch](https://python.langchain.com/docs/integrations/vectorstores/docarray_in_memory/#using-docarrayinmemorysearch).\n",
    "\n",
    "Before going on with the transcripts example, lets look at a simple example creating a `DocArrayInMemorySearch` from a list of strings. In my example, I've added strings that are all related to the Boston area in someway, but they are not all closely related.\n",
    "\n",
    "Also, notice that we pass the `embeddings` variable we initialize earlier to the `AzureOpenAIEmbeddings`.\n",
    "\n",
    "Run the following to load the in-memory vector store and have the store get the embeddings for each of the strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"MIT is in Cambridge\",\n",
    "        \"Harvard is in Cambridge\",\n",
    "        \"Harvard is a university\",\n",
    "        \"Cambridge is across the river from Boston\",\n",
    "        \"Beacon Hill is in Boston\",\n",
    "        \"Samuel Adams lived in Boston\",\n",
    "    ],\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's give it a text query to perform a similarity search on the store for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore1.similarity_search_with_score(query=\"Where is MIT?\", k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above did a pretty good job at listing the relevant string at the top.\n",
    "\n",
    "#### Using a Retriever\n",
    "\n",
    "In LangChain [retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers/) are an abstraction from the actual store of documents - this will help us later. We can get a retriever from the vector store and use a syntax that is more familiar to what we did earlier calling the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever1 = vectorstore1.as_retriever()\n",
    "retriever1.invoke(\"Where is MIT?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have most of the pieces we need to have a RAG system for this simple vector store of strings, let's go ahead and add the remaining pieces.\n",
    "\n",
    "The remaining glue is something I honestly am still wrapping my head around and won't try to explain here: `RunnableParrallel` and `RunnablePassthrough`. For a more detailed example of RunnableParallel and RunnablePassthrough check out [Formatting inputs & output](https://python.langchain.com/docs/expression_language/primitives/parallel/).\n",
    "\n",
    "If you're like me, it helps to first see how something is used in order to get an idea of what it does, then figure out the details later.\n",
    "\n",
    "So, here goes. Run the below to create a RunnableParallel that takes two parameters (which will go into that prompt template we created earlier): context and question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())\n",
    "\n",
    "# ask a question\n",
    "setup.invoke(\"Where did Sam Adams live?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks like everything we need to provide the prompt what it needs - ***and*** it has the proper values from the similarity comparison from the vector store.\n",
    "\n",
    "Next, add it to the chain and see what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup | prompt | llm | parser\n",
    "chain.invoke(\"Where did Sam Adams live?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so it responded with a nice answer, but did it just pull it from the context or did the LLM have to reason about what we gave it?\n",
    "\n",
    "Let's try a more complicated question that isn't in the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Where is Cambridge compared to where Sam Adams lived?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's pretty decent. That's a small RAG example. Now back to the transcript which is a little more complicated and interesting to see working.\n",
    "\n",
    "Create an in-memory vector store with the transcripts and pass the embeddings variable to have it get the embeddings from AzureOpenAI.\n",
    "\n",
    "> NOTE: This takes a few seconds (7.5 seconds on my home network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore2 = DocArrayInMemorySearch.from_documents(transcripts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walkthrough the steps we did earlier just to verify things look good.\n",
    "\n",
    "First try a similarity search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore2.similarity_search_with_score(query=\"What is LangChain?\", k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try the retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever2 = vectorstore2.as_retriever()\n",
    "retriever2.invoke(\"What is LangChain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All looks good, so now let's create a chain and try it out.\n",
    "\n",
    "With this example, let's use the other syntax that allows us to skip the step of creating the RunnableParallel ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever2, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is LangChain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the day, you may get a decent answer back ... but you probably won't with the `prompt` template. Try the `prompt_template` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever2, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is LangChain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That prompt gives me a better answer. Keep that in mind: the prompt is **very important**.\n",
    "\n",
    "That is the beginning of a RAG application using the Youtube transcripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "This notebook is a modified version of this notebook: [Building a RAG application from scratch](https://github.com/svpino/youtube-rag/blob/main/rag.ipynb). \n",
    "\n",
    "There is a Youtube video of the same content [Building a RAG application from scratch using Python, LangChain, and the OpenAI API](https://www.youtube.com/watch?v=BrsocJb-fAo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
