{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG Application with LangChain, Part 2\n",
    "\n",
    "In [Lab 2](./2_rag.ipynb) you built a RAG application that allowed you to search for relevant parts of a YouTube video transcript file that was split into 5 minute intervals - that means the context passed to the LLm was blocks of text consisting of 5 minutes of the video's transcript. \n",
    "\n",
    "This lab starts off with setting things up that were explained in lab 2 and then focuses on working with a different transcript file. The file we use has the whole transcript for each video. This is the same challenge you will have if you start loading files (ie. PDF, Docx, etc.) for your RAG application, the content will be larger than the context window allowed by the LLM.\n",
    "\n",
    "Learning Objectives\n",
    "\n",
    "* Learn how to chunk the transcript into smaller sizes\n",
    "* Learn how text chunking size provides different quality retrieval results in a RAG application\n",
    "\n",
    "\n",
    "### Step 1: Setup what we learned in Lab 2\n",
    "\n",
    "Run the following to get ready for this lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "  openai_api_version=\"2023-05-15\",\n",
    "  azure_deployment= os.getenv(\"AZURE_OPENAI_MODEL_DEPLOYMENT_NAME\")\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings()\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are a helpful assistant that is very brief but polite in your answers. Answer questions in less than 50 words.\n",
    "            Answer the question based on the context below. If you can't \n",
    "            answer the question, reply \"I don't know\".\n",
    "\n",
    "            Context: {context}\n",
    "         \"\"\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the transcript file\n",
    "\n",
    "As mentioned above, we are using a different transcript file. We still load this one the same way we did in Lab 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"./prep/output/master.json\"\n",
    "\n",
    "transcripts_dataset = pd.read_json(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the contents you can run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and specify to use the transcript text column for the page content\n",
    "loader = DataFrameLoader(transcripts_dataset, page_content_column=\"text\")\n",
    "transcripts = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the document listing, run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Chunk the transcripts into smaller pieces\n",
    "\n",
    "#### Document Chunking\n",
    "\n",
    "The process of taking a document and splitting into pieces is often referred to as \"chunking\". There are many ways to split a document but you need to keep in mind what each chunk means for your RAG system.\n",
    "\n",
    "Important things to remember about these chunks:\n",
    "\n",
    "* We will get embeddings for each chunk\n",
    "* Relevant chunks will be found by a similarity search using embeddings\n",
    "* Often times an overlap of 10 - 20% is used\n",
    "* We are using the text-embedding-ada-002 embedding model which means each chunk will have an array of 1,536 float number attached to it\n",
    "* When working with real documents, you may need to address tables and images (images typically have different embedding models)\n",
    "* Each chunk needs to fit in the context window of the LLM, and keep in mind things can get [lost in the middle](https://arxiv.org/abs/2307.03172) when the context is too big\n",
    "* You may need to modify your chunking to improve the retrieval quality of your system\n",
    "\n",
    "Let's first start with a simple RecursiveCharacterTextSplitter (it seems to be one of the more popular choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# use a small chunk number for looking at the overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=50)\n",
    "documents_256 = text_splitter.split_documents(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see how many there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents_256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following and take a look at the `page_content` attribute and notice how the text at the end of one is repeated in the beginning of the next document's `page_content`. This is the overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use a different chunk size of 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter2 = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100)\n",
    "documents_512 = text_splitter2.split_documents(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see how many documents there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents_512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load both into in-memory vector stores to see if there is any difference in retrieval quality with them.\n",
    "\n",
    "> NOTE:\n",
    ">\n",
    "> This took more than 1 minute to run on my home machine. You may need to only use a portion of the documents_256, such as documents_256[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_256 = DocArrayInMemorySearch.from_documents(documents_256, embeddings)\n",
    "vectorstore_512 = DocArrayInMemorySearch.from_documents(documents_512, embeddings)\n",
    "\n",
    "retriever_256 = vectorstore_256.as_retriever()\n",
    "retriever_512 = vectorstore_512.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the same text query, get the top 4 most relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs_256 = retriever_256.get_relevant_documents(query=\"What is langchain?\")\n",
    "unique_docs_512 = retriever_512.get_relevant_documents(query=\"What is langchain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the 256 chunk documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs_256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the 51s chunk documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs_512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice they are **not the same document listing** When I run it, there is 1 document that is in both.\n",
    "\n",
    "So now we know chunk sizes effect the similarity search. \n",
    "\n",
    "Next let's continue with this and see how which one creates a better response from the LLM.\n",
    "\n",
    "First try the 256 chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_256 = (\n",
    "    {\"context\": retriever_256, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain_256.invoke(\"What is LangChain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try the 512 chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_512 = (\n",
    "    {\"context\": retriever_512, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain_512.invoke(\"What is LangChain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your results may vary, but the 512 chunk give me a much better response.\n",
    "\n",
    "I'll leave it to you to try other sizes. The next I would try is 1024.\n",
    "\n",
    "You may also want to try one of the other [Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) like the [SemanticChunker](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker/) or [CharacterTextSplitter using tokens](https://python.langchain.com/docs/modules/data_connection/document_transformers/split_by_token/) to see how they effect the results of retrieval.\n",
    "\n",
    "### Reference\n",
    "\n",
    "TODO: Add some useful links here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
